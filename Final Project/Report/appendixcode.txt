knitr::opts_chunk$set(echo = FALSE)
# read in the data, create dataframe
gz = gzfile('data/covtype.data.gz','rt') 
forest.orig = read.csv(gz,header=F)
forest.orig.colnames = t(read.csv('data/covtyp.colnames.csv',header=F))
colnames(forest.orig) = forest.orig.colnames

# identify continuous variables
forest.var.continuous = c('Elevation','Aspect','Slope', 'HDist.Hydrology', 'VDist.Hydrology', 
                          'HDist.Roadway', 'Hillshade.9am', 'Hillshade.12pm', 'Hillshade.3pm',
                          'HDist.FirePoint')

# for speed, will perform eda on subset until ready to do a full run
set.seed(33)
forest = forest.orig[sample(nrow(forest.orig),20000),]
forest.var.discrete.indices = grep("^Area|^SoilType|CoverType", colnames(forest))
forest[,forest.var.discrete.indices] = as.factor(unlist(forest[,forest.var.discrete.indices]))

forest.numeric = as.data.frame(sapply(forest,as.numeric))

covertype.names = c('Spruce-fir','Lodgepole Pine','Ponderosa Pine','Cottonwood-Willow','Aspen','Douglas-fir','Krummholz')

forest$CoverType[forest$CoverType==1] = 'Spruce-fir'
forest$CoverType[forest$CoverType==2] = 'Lodgepole Pine'
forest$CoverType[forest$CoverType==3] = 'Ponderosa Pine'
forest$CoverType[forest$CoverType==4] = 'Cottonwood-Willow'
forest$CoverType[forest$CoverType==5] = 'Aspen'
forest$CoverType[forest$CoverType==6] = 'Douglas-fir'
forest$CoverType[forest$CoverType==7] = 'Krummholz'


# add Area column
# are any in multiple areas? NO. all belong to only one area
idx = grep("Area", colnames(forest))
temp = forest.numeric[,idx]
temp.rows = temp[apply(temp,1,sum) > 1,]
temp$Z.Area = apply(temp,1,function(x) {
  return(which.max(x))
})
forest.numeric$Z.Area = temp$Z.Area

forest.scaled = as.data.frame(scale(forest.numeric))

library(lattice)
library(ggplot2)
library(corrplot)
library(MASS)
library(caret)
library(car)
library(DMwR)
library(dplyr)
library(randomForest)
library(nnet)
library(neuralnet)

cover_types <- c("Spruce/Fir", "Lodgepole Pine", "Ponderosa Pine", "Cottonwood/Willow", "Aspen", 
                 "Douglas/Fir", "Krummholz")
instances <- c(211840, 283301, 35754, 2747, 9493, 17367, 20510)
ct.df <- data.frame(cover_types, instances)
colnames(ct.df) <- c("Cover Type", "Number of Observations")
knitr::kable(ct.df, caption = "Cover Type Classes", format="pandoc") 
features <- c("Elevation", "Aspect", "Slope", "HDist.Hydrology", "VDist.Hydrology",
              "HDist.Roadway", "Hillshade.9am", "Hillshade.12pm", "Hillshade.3pm", 
              "HHDist.FirePoint", "Area", "SoilType")
descriptions <- c("Elevation in meters", "Aspect in degrees aziumuth", "Slope in degrees", 
                 "Horizontal distance to nearest surface water feature in meters", 
                 "Vertical distance to nearest surface water feature in meters", 
                 "Horizontal distance to nearest roadway in meters", "Hillshade index at 9am, summer solstice", 
                "Hillshade index at noon, summer soltice", "Hillshade index at 3pm, summer solstice", 
              "Horizontal Distance to nearest wildfire ignition points", "Wilderness area designation - 4 binary areas",
              "Soil Type designation - 40 binary values")
features.df <- data.frame(features, descriptions)
colnames(features.df) <- c("Feature", "Descriptions")
knitr::kable(features.df, caption = "Features", format="pandoc") 
library(scales)

ggplot(as.data.frame(table(forest.orig$CoverType)), aes(x=Var1, y = Freq)) + ggtitle("Forest Cover
Frequency by Class") + geom_bar(stat = "identity", fill="#1f78b4", width=.5,
                                color="black") + xlab("Cover Type") + scale_y_continuous(name="Frequency", labels = comma)
options(digits=3)
my.summary <- function(x,...){
  c(mean=round(mean(x, ...), digits = 4),
    sd=round(sd(x, ...), digits=4),
    median=median(x, ...),
    min=min(x, ...),
    max=max(x,...),
    nmiss=sum(is.na(x,...)),
    type="Continuous")
}


forest.stats= apply(forest.orig[,1:10], 2, my.summary)

library(knitr)
kable(t(forest.stats), caption= "Summary Statistics for Continuous Data", format="pandoc")
## area
forest.area= forest.orig[11:14]

summary.area <- data.frame(
  Name = character(),
  Count = numeric(),
  stringsAsFactors = F)

for (i in 1:4){
  summary.area[i,1] <- names(forest.area[i])
  summary.area[i,2] <- sum(forest.area[,i])
}

area<-summary.area[with(summary.area,order(-Count)),]
kable(area,caption= "Area Type Counts", format="pandoc", row.names = F)

## soil
forest.soil= forest.orig[15:54]

summary.soil <- data.frame(
  Name = character(),
  Count = numeric(),
  stringsAsFactors = F)

for (i in 1:40){
  summary.soil[i,1] <- names(forest.soil[i])
  summary.soil[i,2] <- sum(forest.soil[,i])
}


soil<-summary.soil[with(summary.soil,order(-Count)),]
kable(soil, caption= "Soil Type Counts", format="pandoc", row.names = F)

st = stack(as.data.frame(forest.scaled[,forest.var.continuous]))
ggplot(as.data.frame(st)) +
  geom_boxplot(aes(x = ind, y = values)) +
  theme(axis.text.x = element_text(angle=45, hjust = 1)) +
  scale_x_discrete(name ="") + scale_y_continuous(name ="") +
  ggtitle("Boxplots of Scaled Continuous Variables")
density.plots = densityplot(~ Elevation + Aspect + Slope + HDist.Hydrology + VDist.Hydrology + 
                              HDist.Roadway + Hillshade.9am + Hillshade.12pm + Hillshade.3pm +
                              HDist.FirePoint,
                            data=forest, 
                            groups = CoverType, 
                            plot.points = FALSE, 
                            auto.key = list(space="right",title="Cover Type",cex=.6),
                            scales= list(x="free",y="free"), 
                            xlab = '', 
                            ylab=list(cex=.8), 
                            aspect="fill",
                            par.strip.text=list(cex=.9))
plot(density.plots)
density.plots = densityplot(~ Elevation + Aspect + Slope + HDist.Hydrology + VDist.Hydrology + 
                              HDist.Roadway + Hillshade.9am + Hillshade.12pm + Hillshade.3pm +
                              HDist.FirePoint,
                            data=forest.numeric, 
                            groups = Z.Area, 
                            plot.points = FALSE, 
                            auto.key = list(space="right",title="Area",cex=.6),
                            scales= list(x="free",y="free"), 
                            xlab = '', 
                            ylab=list(cex=.8), 
                            aspect="fill",
                            par.strip.text=list(cex=.9))
plot(density.plots)
corrplot(cor(forest[, forest.var.continuous]), 
         tl.col = "black", tl.cex = 0.8, tl.srt = 45,
         cl.cex = 0.8, pch.cex = 0.8, diag = FALSE,
         type="lower",
         addCoefasPercent = TRUE, addCoef.col = TRUE,number.cex = .6) #Matt added to show correlation amounts
idx = grep("SoilType|CoverType", colnames(forest.numeric))
df = as.data.frame(forest.numeric[,idx])
idx.type = grep("CoverType", colnames(df))

df.temp = df[,-idx.type]

soil.sums = apply(df.temp,2,function(x) {
  tbl = table(x,df$CoverType)
  if (dim(tbl)[1] < 2) {
    tbl = rbind('0' = tbl, '1' = rep(0,7), deparse.level = 1)
  }
  return (apply(tbl,1,sum)[2])
})
#soil.sums

soil.sums.byclass = apply(df[,-idx.type],2,function(x) {
  tbl = table(x,df$CoverType)
  tbl = tbl[seq(2,14,by=2)]
  return (tbl)
})
#soil.sums.byclass

soil.ratios = as.data.frame(t(soil.sums.byclass)/soil.sums)
library(RColorBrewer)
soil.ratios.m = na.omit(as.matrix(soil.ratios))
barchart(soil.ratios.m,col=brewer.pal(7, "Pastel2"),xlab='',
         key=list(space="right",
                  lines=list(col=brewer.pal(7, "Pastel2"),lwd=3),
                  text=list(covertype.names)
))

idx = grep("Area1|Area2|Area3|Area4|CoverType", colnames(forest.numeric))
df = as.data.frame(forest.numeric[,idx])
idx.type = grep("CoverType", colnames(df))

df.temp = df[,-idx.type]

area.sums = apply(df.temp,2,function(x) {
  tbl = table(x,df$CoverType)
  if (dim(tbl)[1] < 2) {
    tbl = rbind('0' = tbl, '1' = rep(0,7), deparse.level = 1)
  }
  return (apply(tbl,1,sum)[2])
})
#area.sums

area.sums.byclass = apply(df[,-idx.type],2,function(x) {
  tbl = table(x,df$CoverType)
  tbl = tbl[seq(2,14,by=2)]
  return (tbl)
})
#area.sums.byclass

area.ratios = as.data.frame(t(area.sums.byclass)/area.sums)
library(RColorBrewer)
area.ratios.m = na.omit(as.matrix(area.ratios))
barchart(area.ratios.m,col=brewer.pal(7, "Pastel2"),xlab='',
         key=list(space="right",
                  lines=list(col=brewer.pal(7, "Pastel2"),lwd=3),
                  text=list(covertype.names)
                  )
)


ggplot(forest, aes(x=HDist.Roadway)) + 
  geom_histogram(aes(group=CoverType, colour=CoverType, fill=CoverType), bins=30, alpha=0.7)+
  ggtitle('')+
  theme(legend.title = element_blank())+
  labs(x="Distance to Roadway",y="Count")
ggplot(forest, aes(x=HDist.Hydrology)) + 
  geom_histogram(aes(group=CoverType, colour=CoverType, fill=CoverType), bins=30, alpha=0.7)+
  ggtitle('')+
  theme(legend.title = element_blank())+
  labs(x="Distance to Water",y="Count")
ggplot(forest, aes(x=Aspect)) + 
    geom_point(aes(y=Hillshade.9am, color="Hillshade.9am"), alpha=.1) +
    geom_point(aes(y=Hillshade.3pm, color="Hillshade.3pm"), alpha=.1)
# transform Hillshade.12pm and Vdist.Hydrology
forest.new= forest.orig

#forest.new$trans.Hillshade.12pm = log(forest.new$Hillshade.12pm)

#forest.new$trans.VDist.Hydrology = preProcess(forest.new$VDist.Hydrology, method = "YeoJohnson")
forest.new$trans.LDist.Hydrology <- sqrt(forest.new$VDist.Hydrology^2 + forest.new$HDist.Hydrology^2) 


# Soil Type to Climate Zone Mapping
forest.new$trans.zone27 <- rowSums(forest.new[,c("SoilType1", "SoilType2","SoilType3","SoilType4", "SoilType5","SoilType6")])
forest.new$trans.zone35 <- rowSums(forest.new[,c("SoilType7", "SoilType8")])
forest.new$trans.zone42 <- forest.new$SoilType9
forest.new$trans.zone47 <- rowSums(forest.new[,c("SoilType10", "SoilType11","SoilType12","SoilType13")])
forest.new$trans.zone51 <- rowSums(forest.new[,c("SoilType14", "SoilType15")])
forest.new$trans.zone61 <- rowSums(forest.new[,c("SoilType16", "SoilType17")])
forest.new$trans.zone67 <- forest.new$SoilType18
forest.new$trans.zone71 <- rowSums(forest.new[,c("SoilType19", "SoilType20","SoilType21")])
forest.new$trans.zone72 <- rowSums(forest.new[,c("SoilType22", "SoilType23")])
forest.new$trans.zone77 <- rowSums(forest.new[,c("SoilType24", "SoilType25","SoilType26","SoilType27", "SoilType28",
  "SoilType29", "SoilType30","SoilType31","SoilType32", "SoilType33", "SoilType34")])
forest.new$trans.zone87 <- rowSums(forest.new[,c("SoilType35", "SoilType36","SoilType37","SoilType38", "SoilType39","SoilType40")])



#PCA
set.seed(444)
idxs.soil = grep("Soil", colnames(forest.new))
temp.soil = forest.new[,idxs.soil]
soil.pca = prcomp(temp.soil, scale = F)
soil.pca.x = soil.pca$x
colnames(soil.pca.x) <- c(paste0("soil.pca.pc",as.character(1:ncol(soil.pca.x))))
 #summary(soil.pca)$importance[3,]
forest.new <- cbind(forest.new, soil.pca.x[,1:16]) #90.7% of importantance

idxs.zone = grep("trans.zone", colnames(forest.new))
temp.zone = forest.new[,idxs.zone]
zone.pca = prcomp(temp.zone, scale = F)
zone.pca.x = zone.pca$x
colnames(zone.pca.x) <- c(paste0("zone.pca.pc",as.character(1:ncol(zone.pca.x))))
 #summary(zone.pca)$importance[3,]
forest.new <- cbind(forest.new, zone.pca.x[,1:6]) #99.7% of importantance

par(mfrow=c(1,2))
plot(summary(soil.pca)$importance[3,], xlab="Principal Components", ylab="Importance",
     main="Soil Type PCA Importance Plot", pch=20, col="red")
plot(summary(zone.pca)$importance[3,], xlab="Principal Components", ylab="Importance",
     main="Soil Zone PCA Importance Plot", pch=20, col="blue")
par(mfrow=c(1,1))

#rm(list = c(temp.soil, temp.zone))


idxs = grep("Aspect|Hillshade|Slope", colnames(forest.numeric))
temp = forest.new[,idxs]
shadeslope.pca = prcomp(temp, scale = F)
shadeslope.pca.x = shadeslope.pca$x
colnames(shadeslope.pca.x) = c('shade.pca.pc1','shade.pca.pc2','shade.pca.pc3','shade.pca.pc4','shade.pca.pc5')
forest.new <- cbind(forest.new, shadeslope.pca.x[,1:3]) #99.7% of important
#plot(summary(shadeslope.pca)$importance[3,])
#summary(shadeslope.pca)$importance[3,]
#summary(shadeslope.pca)

idxs = grep("pca|PC", colnames(forest.new))
corrplot(cor(forest.new[,idxs]), 
         tl.col = "black", tl.cex = 0.8, tl.srt = 45,
         cl.cex = 0.8, pch.cex = 0.8, diag = FALSE,
         type="lower",
         addCoefasPercent = TRUE, addCoef.col = TRUE,number.cex = .6) #Matt added to show correlation amounts



#idxs.nzv = nzv(forest.new)
#colnames(forest.new)[idxs.nzv]

idxs_trans = grep("trans", colnames(forest.new))
forest.transforms= forest.new[,idxs_trans]
idxs_pca = grep("PC|shade.pca", colnames(forest.new))
forest.pca= forest.new[,idxs_pca]
idxs_buck = grep("buck", colnames(forest.new))
forest.bucketing= forest.new[,idxs_buck]

#Training/Test Splitting
set.seed(330)
fraction.train <- .7 # Enter Training Set Size
fraction.valid <- 1 - fraction.train
size.train <- fraction.train*nrow(forest.new)
size.valid <- nrow(forest.new) - size.train
indices.train <- sort(sample(seq_len(nrow(forest.new)), size=size.train))
indices.valid <- setdiff(seq_len(nrow(forest.new)), indices.train)
forest.train <- forest.new[indices.train,]
forest.valid <- forest.new[indices.valid,]


#Remove SoilTypes and Aspect, Slope and Hillsides
forest.train.rm <- forest.train[,-c(2,3,4,7,8,9, 15:54,68:83)]
forest.train.rm <- forest.train.rm[,-5]
forest.train.rm$CoverType <- as.factor(forest.train.rm$CoverType)
## 
## #explore.lda <- train(CoverType ~ .,  data = forest.train.rm,
##                              #method = "lda",
##                              #metric = "Kappa",
##                              #preProc = c("center", "scale"))
## 
#saveRDS(explore.lda, "./explore.lda.rds")

explore.lda <- readRDS("./explore.lda.rds")
kable(explore.lda$finalModel$scaling, format="pandoc")
library(rpart)
library(rpart.plot)
rpart.plot(rpart(factor(forest.train.rm$CoverType) ~ ., data = forest.train.rm), main = "Tree Plot for Cover Type")
## library(randomForest)
## set.seed(123)
## explore.rf <- randomForest(CoverType~.,
##                          data = forest.train.rm,
##                          ntree = 10, nodesize = 10, importance = TRUE)
#saveRDS(explore.rf, "./explore.rf.rds")

explore.rf <- readRDS("./explore.rf.rds")
library(randomForest)
varImpPlot(explore.rf, main = 'Variable Importance', cex = 0.5,pch=19, col=1)
## 
## # read in the data, create dataframe
## gz = gzfile('data/covtype.data.gz','rt')
## forest.orig = read.csv(gz,header=F)
## forest.orig.colnames = t(read.csv('data/covtyp.colnames.csv',header=F))
## colnames(forest.orig) = forest.orig.colnames
## forest.var.continuous = c('Elevation','Aspect','Slope', 'HDist.Hydrology', 'VDist.Hydrology',
##                           'HDist.Roadway', 'Hillshade.9am', 'Hillshade.12pm', 'Hillshade.3pm',
##                           'HDist.FirePoint')
## library(lattice)
## library(ggplot2)
## library(corrplot)
## library(MASS)
## library(caret)
## library(car)
## library(DMwR)
## library(dplyr)
## library(doMC)
## library(randomForest)
## library(nnet)
## library(neuralnet)
## registerDoMC(cores = 5)
## 
## forest.new= forest.orig
## forest.new$trans.LDist.Hydrology <- sqrt(forest.new$VDist.Hydrology^2 + forest.new$HDist.Hydrology^2)
## 
## forest.new$trans.zone27 <- rowSums(forest.new[,c("SoilType1", "SoilType2","SoilType3","SoilType4", "SoilType5","SoilType6")])
## forest.new$trans.zone35 <- rowSums(forest.new[,c("SoilType7", "SoilType8")])
## forest.new$trans.zone42 <- forest.new$SoilType9
## forest.new$trans.zone47 <- rowSums(forest.new[,c("SoilType10", "SoilType11","SoilType12","SoilType13")])
## forest.new$trans.zone51 <- rowSums(forest.new[,c("SoilType14", "SoilType15")])
## forest.new$trans.zone61 <- rowSums(forest.new[,c("SoilType16", "SoilType17")])
## forest.new$trans.zone67 <- forest.new$SoilType18
## forest.new$trans.zone71 <- rowSums(forest.new[,c("SoilType19", "SoilType20","SoilType21")])
## forest.new$trans.zone72 <- rowSums(forest.new[,c("SoilType22", "SoilType23")])
## forest.new$trans.zone77 <- rowSums(forest.new[,c("SoilType24", "SoilType25","SoilType26","SoilType27",
##                                                  "SoilType28", "SoilType29", "SoilType30","SoilType31",
##                                                  "SoilType32", "SoilType33", "SoilType34")])
## forest.new$trans.zone87 <- rowSums(forest.new[,c("SoilType35", "SoilType36","SoilType37","SoilType38",
##                                                  "SoilType39","SoilType40")])
## 
## set.seed(444)
## idxs.soil = grep("Soil", colnames(forest.new))
## temp.soil = forest.new[,idxs.soil]
## soil.pca = prcomp(temp.soil, scale = F)
## soil.pca.x = soil.pca$x
## colnames(soil.pca.x) <- c(paste0("soil.pca.pc",as.character(1:ncol(soil.pca.x))))
## forest.new <- cbind(forest.new, soil.pca.x[,1:16]) #90.7% of importantance
## idxs.zone = grep("trans.zone", colnames(forest.new))
## temp.zone = forest.new[,idxs.zone]
## zone.pca = prcomp(temp.zone, scale = F)
## zone.pca.x = zone.pca$x
## colnames(zone.pca.x) <- c(paste0("zone.pca.pc",as.character(1:ncol(zone.pca.x))))
## forest.new <- cbind(forest.new, zone.pca.x[,1:6]) #99.7% of importantance
## rm(list = c('temp.soil', 'temp.zone'))
## 
## idxs = grep("Aspect|Hillshade|Slope", colnames(forest.new))
## temp = forest.new[,idxs]
## shadeslope.pca = prcomp(temp, scale = F)
## shadeslope.pca.x = shadeslope.pca$x
## colnames(shadeslope.pca.x) = c('shade.pca.pc1','shade.pca.pc2','shade.pca.pc3','shade.pca.pc4','shade.pca.pc5')
## forest.new <- cbind(forest.new, shadeslope.pca.x[,1:3]) #99.7% of important
## 
## idxs_trans = grep("trans", colnames(forest.new))
## forest.transforms= forest.new[,idxs_trans]
## idxs_pca = grep("PC|shade.pca", colnames(forest.new))
## forest.pca= forest.new[,idxs_pca]
## #idxs_buck = grep("buck", colnames(forest.new))
## #forest.bucketing= forest[,idxs_buck]
## 
## rm(list = c('temp.soil', 'temp.zone', 'forest.pca', 'forest.transforms', 'shadeslope.pca.x',
##             'forest.bucketing', 'soil.pca.x', 'temp', 'zone.pca.x', 'forest', 'forest.numeric',
##             'forest.pca', 'idxs_pca', 'idxs_buck', 'idxs_trans', 'idxs.soil', 'idxs.zone',
##             'shadeslope.pca', 'soil.pca', 'zone.pca', 'gz'))
## 

forest.model = forest.new

### update forest.new with any renames or consolidations
# rename some predictors
forest.model.colnames = colnames(forest.model)
idxs = grep("^PC",forest.model.colnames)
forest.model.colnames[idxs] = paste("pca.soil.",forest.model.colnames[idxs],sep = "")
colnames(forest.model) = forest.model.colnames

# convert 4 area predictors to one factor variable
idxs = grep("^Area",colnames(forest.model))
mat = as.matrix(forest.model[,idxs])
forest.model$forest.area = factor(mat%*%(1:ncol(mat)), labels = colnames(mat)) 
mat = NULL
forest.model[,idxs] = NULL

# convert zones into one factor variable
idxs = grep("trans.zone",colnames(forest.model))
mat = as.matrix(forest.model[,idxs])
forest.model$forest.zone = factor(mat%*%(1:ncol(mat)), labels = colnames(mat)) 
mat = NULL
forest.model[,idxs] = NULL

# convert to factors where needed
idxs = grep("Area|SoilType|CoverType|trans.zone",colnames(forest.model))
forest.model[,idxs] = lapply(forest.model[,idxs], as.factor)

#Training/Test Splitting
set.seed(330)
fraction.train = .7 # Enter Training Set Size
fraction.valid = 1 - fraction.train
size.train = fraction.train*nrow(forest.model)
size.valid = nrow(forest.model) - size.train
indices.train = sort(sample(seq_len(nrow(forest.model)), size=size.train))
indices.valid = setdiff(seq_len(nrow(forest.model)), indices.train)

# create a couple versions of the model to try out with the algorithms
forest.model1 = forest.model # bare bones model without Soil anything
forest.model2 = forest.model # bare bones model with Soil pca

# remove the unwanted variables for modeling
idxs = grep("^SoilType|^Aspect|^Hillshade|^Slope|^HDist.Hydrology|VDist.Hydrology|soil.pca|zone.pca",colnames(forest.model))
forest.model1[,idxs] = NULL

idxs = grep("^Aspect|^Hillshade|^Slope|^HDist.Hydrology|VDist.Hydrology|^SoilType|zone.pca",colnames(forest.model))
forest.model2[,idxs] = NULL

forest.train1 = forest.model1[indices.train,]
forest.valid1 = forest.model1[indices.valid,]

forest.train2 = forest.model2[indices.train,]
forest.valid2 = forest.model2[indices.valid,]

# model that makes sure the low model types are always present in a sample
set.seed(444)
sample4.tbl = table(forest.model2$CoverType)
forest.model4 = sample_n(forest.model2[forest.model2$CoverType==4,], 1917)
for (i in 1:7) {
  cnt = sample4.tbl[i]
  if (cnt > 100000) {
    forest.model4 = rbind(forest.model4, sample_n(forest.model2[forest.model2$CoverType==i,],20000))
  } else if (cnt > 10000) {
    forest.model4 = rbind(forest.model4, sample_n(forest.model2[forest.model2$CoverType==i,],5000))
  } else if (cnt > 3000) {
    forest.model4 = rbind(forest.model4, sample_n(forest.model2[forest.model2$CoverType==i,],3000))
  }
}
forest.train4 = forest.model4
forest.valid4 = forest.model2[indices.valid,]


rm(list = c('indices.train', 'indices.valid', 'size.train', 'size.valid'))

## 
## ### Check if sampling technqiues will improve accuracy
## 
## # percentage by class before sampling
## forest.train.proptable = round(prop.table(table(forest.train$CoverType)), digits = 3)
## forest.train.proptable
## 
## # train set 1: downsample all classes to lowest observation class count.
## # 1,917 each, 13,300 total
## set.seed(444)
## forest.sample1 = as.data.frame(forest.train %>% group_by(CoverType) %>% sample_n(size = 1917))
## sample1.forest = randomForest(CoverType~.,data=forest.sample1,importance=TRUE)
## sample1.pred = caret::predict(sample1.forest, newdata = forest.valid)
## sample1.cm = confusionMatrix(sample1.pred,forest.valid$CoverType)
## sample1.cm$overall[2]
## rm(list = c('forest.sample1','sample1.forest','sample1.pred'))
## 
## # random sample of 13,300
## set.seed(444)
## sample2.forest = randomForest(CoverType~.,data=sample_n(forest.train,13300),importance=TRUE)
## sample2.pred = predict(sample2.forest, newdata = forest.valid)
## sample2.cm = confusionMatrix(sample2.pred, forest.valid$CoverType)
## sample2.cm$overall[2]
## rm(list = c('sample2.forest','sample2.pred'))
## 
## 
## # 5000 of each class with replacement
## set.seed(444)
## forest.sample3 = as.data.frame(forest.train %>% group_by(CoverType) %>% sample_n(size = 5000,replace=TRUE))
## sample3.forest = randomForest(CoverType~.,data=forest.sample3,importance=TRUE)
## sample3.pred = predict(sample3.forest, newdata = forest.valid)
## sample3.cm = confusionMatrix(sample3.pred, forest.valid$CoverType)
## sample3.cm$overall[2]
## rm(list = c('forest.sample3','sample3.forest','sample3.pred'))
## 
## 
## # tiered selection of samples based on category
## sample4.tbl = table(forest.train$CoverType)
## forest.sample4 = sample_n(forest.train[forest.train$CoverType==4,], 1917)
## for (i in 1:7) {
##   cnt = sample4.tbl[i]
##   if (cnt > 100000) {
##     forest.sample4 = rbind(forest.sample4, sample_n(forest.train[forest.train$CoverType==i,],20000))
##   } else if (cnt > 10000) {
##     forest.sample4 = rbind(forest.sample4, sample_n(forest.train[forest.train$CoverType==i,],5000))
##   } else if (cnt > 3000) {
##     forest.sample4 = rbind(forest.sample4, sample_n(forest.train[forest.train$CoverType==i,],3000))
##   }
## }
## sample4.forest = randomForest(CoverType~., data=forest.sample4, importance=TRUE)
## sample4.pred = predict(sample4.forest, newdata = forest.valid)
## sample4.cm = confusionMatrix(sample4.pred, forest.valid$CoverType)
## sample4.cm$overall[2]
## rm(list = c('sample4.tbl','forest.sample4','sample4.forest','sample4.pred'))
## 
## 
## # random sample of 39,917
## set.seed(444)
## #sample5.forest = randomForest(CoverType~., data=sample_n(forest.train, 39917), importance=TRUE)
## sample5.forest = randomForest(CoverType~., data=sample_n(forest.train, 40000), importance=TRUE)
## sample5.pred = predict(sample5.forest, newdata = forest.valid)
## sample5.cm = confusionMatrix(sample5.pred, forest.valid$CoverType)
## sample5.cm$overall[2]
## 
## 
## sample.types = c('Sample1','Sample2','Sample3','Sample4','Sample5')
## sample.results = c(sample1.cm$overall[2],sample2.cm$overall[2],sample3.cm$overall[2],
##                    sample4.cm$overall[2],sample5.cm$overall[2])
## sample.results = c(sample1.cm$overall[1],sample2.cm$overall[1],sample3.cm$overall[1],
##                    sample4.cm$overall[1],sample5.cm$overall[1])
## cbind(sample.types,sample.results)
## 
## # SMOTE sample set
## #set.seed(444)
## #idxs.row = forest.t$CoverType == 1 | forest.t$CoverType == 2
## #forest.t3 = forest.t
## #forest.t3$RelativeSize = as.factor(ifelse(idxs.row,'Majority','Minority'))
## #m = SMOTE(RelativeSize ~ ., forest.t3, perc.over = 50, perc.under=5)
## #table(m$CoverType)
## #temp = table(forest.t$CoverType)
## #forest.t3 = sample_n(forest.v[forest$CoverType==2,],5000)
## 
## # sample of 20,300
## #set.seed(444)
## #df.rf = randomForest(CoverType~.,data=sample_n(forest.t,20300),importance=TRUE)
## #pred = predict(df.rf, newdata = forest.v)
## #confusionMatrix(pred,forest.v$CoverType) # kappa 0.7179
## 
## m = sample4.cm$table
## ms = apply(m,2,sum)
## mss = round(apply(m,1,function(x) {return (x/ms)}),2)
## mss
## 
## corrplot(mss,
##          tl.col = "black",
##          tl.cex = 0.8,
##          tl.srt = 0,
##          cl.cex = 0.8,
##          pch.cex = 0.8,
##          diag = TRUE,
##          type="full",
##          addCoefasPercent = TRUE,
##          addCoef.col = TRUE,
##          number.cex = .6) #Matt added to show correlation amounts
## 
library(nnet)

build = FALSE

## NN1 ##########################################
# model without soil info, 5 nodes, 0.01 decay
set.seed(444)
forest.model.nn1 = sample_n(forest.train1, 40000)
idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.nn1))
forest.model.nn1[,idxs] = scale(forest.model.nn1[,idxs], center = TRUE, scale = TRUE)

if (build) {
  forest.nn1 = nnet(CoverType ~ . , data=forest.model.nn1, linout=FALSE, size=5, maxit=2000, decay=0.01, trace=FALSE)
  saveRDS(forest.nn1, "./forest.nn1")
} else {
  forest.nn1 = readRDS("./forest.nn1")
}

idx = grep("CoverType", colnames(forest.valid1))
forest.model.nn1.valid = forest.valid1[,-idx]
idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.nn1.valid))
forest.model.nn1.valid[,idxs] = scale(forest.model.nn1.valid[,idxs], center = TRUE, scale = TRUE)
forest.nn1.pred = predict(forest.nn1, newdata=forest.model.nn1.valid, type="class")
forest.nn1.cm = caret::confusionMatrix(forest.nn1.pred, reference=forest.valid1$CoverType)
forest.nn1.kappa = forest.nn1.cm$overall[2]
forest.nn1.accuracy = forest.nn1.cm$overall[1]

## NN2 ##########################################
#forest.model.nn2 = sample_n(forest.train2, 40000)
#n = names(forest.train2)
#f = as.formula(paste("CoverType ~", paste(n[!n %in% "CoverType"], collapse = " + ")))
#forest.nn2 = pcaNNet(f, forest.model.nn2, size = 5, linout = FALSE, trace = FALSE)

#model with soil info, 5 nodes, 0.01 decay
set.seed(444)
forest.model.nn2 = sample_n(forest.train2, 40000)

if (build) {
  forest.nn2 = nnet(CoverType ~ . , data=forest.model.nn2, linout=FALSE, size=5, maxit=2000, decay=0.01, trace=FALSE)
  saveRDS(forest.nn2, "./forest.nn2")
} else {
  forest.nn2 = readRDS("./forest.nn2")
}

idx = grep("CoverType", colnames(forest.valid2))
forest.nn2.pred = predict(forest.nn2, newdata=forest.valid2[,-idx], type="class")
forest.nn2.cm = caret::confusionMatrix(data=forest.nn2.pred, reference=forest.valid2$CoverType)
forest.nn2.kappa = forest.nn2.cm$overall[2]
forest.nn2.accuracy = forest.nn2.cm$overall[1]

## NN3 ##########################################
#model with soil info, 9 nodes, 0.01 decay
set.seed(444)
forest.model.nn3 = sample_n(forest.train2, 40000)

if (build) {
  forest.nn3 = nnet(CoverType ~ . , data=forest.model.nn3, linout=FALSE, size=9, maxit=2000, decay=0.01, trace=FALSE)
  saveRDS(forest.nn2, "./forest.nn3")
} else {
  forest.nn3 = readRDS("./forest.nn3")
}

idx = grep("CoverType", colnames(forest.valid2))
forest.nn3.pred = predict(forest.nn3, newdata=forest.valid2[,-idx], type="class")
forest.nn3.cm = caret::confusionMatrix(data=forest.nn3.pred, reference=forest.valid2$CoverType)
forest.nn3.kappa = forest.nn3.cm$overall[2]
forest.nn3.accuracy = forest.nn3.cm$overall[1]


## NN4 ##########################################
#### try with cross validation on soil info set

forest.model.nn4 = forest.train4

# when playing reduce the training set so it'll run faster
playing = FALSE
if (playing) {
  build = TRUE
  idxs = which(as.numeric(forest.model.nn4$CoverType)>2, arr.ind = TRUE)
  temp = sample_n(forest.model.nn4[idxs,], 500)  
  idxs = which(as.numeric(forest.model.nn4$CoverType)<=2, arr.ind = TRUE)
  temp = rbind(temp,sample_n(forest.model.nn4[idxs,], 2000))
  forest.model.nn4 = temp
}

if (build) {
  forest.model.nn4$CoverType = paste0("CoverType",forest.model.nn4$CoverType,sep="")
  forest.model.nn4$CoverType = as.factor(forest.model.nn4$CoverType)
  ctrl = trainControl(method = "cv", number = 10, savePredictions = TRUE, classProbs = TRUE)
  tuneGrid=expand.grid(size=c(9,15,19), decay=c(0.01,0.025,0.05))
  forest.nn4 = train(CoverType ~ .,  
                    data=forest.model.nn4, 
                    #preProcess = c('center','scale'),
                    method="nnet", 
                    trControl = ctrl,
                    tuneGrid = tuneGrid,
                    linout = FALSE,
                    maxit=2000
                    )
  saveRDS(forest.nn4, "./forest.nn4")
} else {
  forest.nn4 = readRDS("./forest.nn4")
}

forest.model.nn4.valid = forest.valid4
idx = grep("CoverType", colnames(forest.valid4))
forest.model.nn4.valid = forest.model.nn4.valid[,-idx]
# need to break out factors to columns
#idxs = grep("forest.zone", colnames(forest.nn4.model))
m = dummyVars(" ~ .", data = forest.model.nn4.valid)
m = predict(m, newdata = forest.model.nn4.valid)
# remove one each of the factor predictors, but leave all of the CoverType factors
n = colnames(m)
idxs = grep("forest.area.Area1|forest.zone.trans.zone27", n)
m = m[,-idxs]
colnames(m) = gsub("area.Area","areaArea",colnames(m))
colnames(m) = gsub("zone.trans","zonetrans",colnames(m))

forest.nn4.pred = predict(forest.nn4$finalModel, newdata=m, type="class")
#table(forest.nn4.pred)
forest.nn4.pred = gsub("CoverType","",forest.nn4.pred)

forest.nn4.cm = caret::confusionMatrix(data=forest.nn4.pred, reference=forest.valid4$CoverType)
forest.nn4.kappa = forest.nn4.cm$overall[2]
forest.nn4.accuracy = forest.nn4.cm$overall[1]

#table(forest.nn4.pred)
#forest.nn4.kappa


## NN5 ##########################################
############################################################
#model with soil info and assured low class observations with all training
#data using best result from CV analysis on smaller training data set
#15 nodes, 0.05 decay
set.seed(444)
forest.model.nn5 = forest.train4

#idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.nn5))
#forest.model.nn5[,idxs] = scale(forest.model.nn5[,idxs], center = TRUE, scale = TRUE)
if (build) {
  forest.nn5 = nnet(CoverType ~ . , data=forest.model.nn5, linout=FALSE, size=15, maxit=2000, decay=0.05, trace=FALSE)
  saveRDS(forest.nn5, "./forest.nn5")
} else {
  forest.nn5 = readRDS("./forest.nn5")
}
forest.model.nn5.valid = forest.valid4
idx = grep("CoverType", colnames(forest.valid4))
forest.model.nn5.valid = forest.model.nn5.valid[,-idx]
#idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.nn5.valid))
#forest.model.nn5.valid[,idxs] = scale(forest.model.nn5.valid[,idxs], center = TRUE, scale = TRUE)
forest.nn5.pred = predict(forest.nn5, newdata=forest.model.nn5.valid, type="class")
forest.nn5.cm = caret::confusionMatrix(data=forest.nn5.pred, reference=forest.valid4$CoverType)
forest.nn5.kappa = forest.nn5.cm$overall[2]
forest.nn5.accuracy = forest.nn5.cm$overall[1]

#table(forest.nn5.pred)
#forest.nn5.kappa


## NN6 ##########################################
#model with soil info, two layer (5,2) nodes, 0.01 decay
library(neuralnet)
set.seed(444)
forest.nn6.model = forest.train4
#forest.nn5.model = forest.nn5.model[1:1000,]
idxs = grep("forest.zone", colnames(forest.nn6.model))
m = dummyVars(" ~ .", data = forest.nn6.model[,-idxs])
m = predict(m, newdata = forest.nn6.model)
# remove one each of the factor predictors, but leave all of the CoverType factors
n = colnames(m)
idxs = grep("forest.area.Area1", n)
m = m[,-idxs]
n = colnames(m)
left = paste(n[grep("CoverType",n)], collapse = " + ")
right = paste(n[-grep("CoverType",n)], collapse = " + ")
fnn = as.formula(paste(left, right, sep = " ~ "))

if (build) {
  #forest.nn6 = neuralnet(fnn, data=m, hidden=c(15,9), linear.output=FALSE, learningrate = 0.5)
  #saveRDS(forest.nn6, "./forest.nn6")
} else {
  #forest.nn6 = readRDS("./forest.nn6")
}

#idxs = grep("forest.zone", colnames(forest.valid4))
#m.test = dummyVars(" ~ .", data = forest.valid4[,-idxs])
#m.test = predict(m.test, newdata = forest.valid4)
#n = colnames(m.test)
#idxs = grep("forest.area.Area1|CoverType", n)
#m.test = m.test[,-idxs]
#forest.nn5.pred = neuralnet::compute(forest.nn5, m.test)$net.result
#head(forest.nn5.pred)

#s = apply(forest.nn5.pred,1,function(x){
#  return(which.is.max(x))
  #return(which(x == max(x), arr.ind = TRUE))
#})


############################################################

set.seed(444)
forest.model.nn7 = forest.train4

#idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.nn7))
#forest.model.nn7[,idxs] = scale(forest.model.nn7[,idxs], center = TRUE, scale = TRUE)
if (build) {
  forest.model.nn7$Y = class.ind(forest.model.nn7$CoverType)
  # delete the old target variable
  forest.model.nn7$CoverType=NULL
  forest.nn7 = nnet(Y ~ . , data=forest.model.nn7, linout=FALSE, size=15, maxit=2000, decay=0.05, trace=FALSE, softmax=TRUE)
  saveRDS(forest.nn7, "./forest.nn7")
} else {
  forest.nn7 = readRDS("./forest.nn7")
}
forest.model.nn7.valid = forest.valid4
idx = grep("CoverType", colnames(forest.valid4))
forest.model.nn7.valid = forest.model.nn7.valid[,-idx]
#idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.nn7.valid))
#forest.model.nn7.valid[,idxs] = scale(forest.model.nn7.valid[,idxs], center = TRUE, scale = TRUE)
forest.nn7.pred = predict(forest.nn7, newdata=forest.model.nn7.valid, type="class")
forest.nn7.cm = caret::confusionMatrix(data=forest.nn7.pred, reference=forest.valid4$CoverType)
forest.nn7.kappa = forest.nn7.cm$overall[2]
forest.nn7.accuracy = forest.nn7.cm$overall[1]

### add code for the best nn performer's accuracy, kappa, f1, and boxplot of f1
#barplot(forest.nn1.cm$byClass[,7])

f1 <- as.data.frame(forest.nn5.cm$byClass[,7])
colnames(f1) <- "F1"
f1$Class <- row.names(f1)
ggplot(data=f1, aes(x=Class, y = F1)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("F1 Scores")
ba <- as.data.frame(forest.nn1.cm$byClass[,11])
colnames(ba) <- "Balanced_Accuracy"
ba$Class <- row.names(ba)
ggplot(data=ba, aes(x=Class, y = Balanced_Accuracy )) +
  geom_bar(stat="identity", width=0.5) + ggtitle("Balanced Accuracy")
forest.4 <- readRDS("forest.4.rds")
set.seed(456)
intrain2<-createDataPartition(y=forest.4$CoverType,p=0.05,list=FALSE)
forest.4.train<-forest.4[intrain2,]
forest.4.test<-forest.4[-intrain2,]
set.seed(7)

# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
# run the RFE algorithm - Backwards Feature Selection with Kappa as metric

#results <- rfe(CoverType ~ ., data = forest.3, metric = "Kappa", rfeControl=control)

#saveRDS(results, "./results.rfe.rds")
results <- readRDS("results.rfe.rds")
# summarize the results
#print(results)
# list the chosen features
rfe.predictors <- predictors(results)
# plot the results
plot(results, type=c("g", "o"))

rfe.pred <- data.frame(rfe.predictors)
kable(rfe.pred, caption= "RFE Predictors", format="pandoc")
control <- trainControl(method="repeatedcv", number=5, repeats=3, search = "random")
metric <- "Kappa"
set.seed(345)
mtry <- sqrt(ncol(forest.4.train))
#rf.mod1 <- train(CoverType~ Elevation + HDist.FirePoint + HDist.Roadway + trans.LDist.Hydrology +
#+  VDist.Hydrology + Hillshade.3pm + Hillshade.12pm + Aspect +   
#+  HDist.Hydrology + interElAr3 + Slope + interElAr1 + SoilType22 +
#+  SoilType23 + SoilType2 + SoilType33, data=forest.4.train, method="rf", 
#+ metric=metric,tuneLength=15, trControl=control)

rf.mod1 <- readRDS("rf.mod1.rds")

plot(rf.mod1)
pred.rf1 = predict(rf.mod1$finalModel, newdata=forest.4.test )
cm.rf1 = confusionMatrix(data=pred.rf1, forest.4.test$CoverType)
library(pander)
pander(cm.rf1$table)
#cm.rf1
f1 <- as.data.frame(cm.rf1$byClass[,7])
colnames(f1) <- "F1"
f1$Class <- row.names(f1)
ggplot(data=f1, aes(x=Class, y = F1)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("F1 Scores")

ba <- as.data.frame(cm.rf1$byClass[,11])
colnames(ba) <- "Balanced_Accuracy"
ba$Class <- row.names(ba)
ggplot(data=ba, aes(x=Class, y = Balanced_Accuracy)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("Balanced Accuracy")
set.seed(444)
idxs.soil = grep("Soil", colnames(forest.4))
temp.soil = forest.4[,idxs.soil]
soil.pca = prcomp(temp.soil, scale = F)
soil.pca.x = soil.pca$x
colnames(soil.pca.x) <- c(paste0("soil.pca.pc",as.character(1:ncol(soil.pca.x))))

forest.4 <- cbind(forest.4, soil.pca.x[,1:16]) #90.7% of importantance

set.seed(456)
intrain3<-createDataPartition(y=forest.4$CoverType,p=0.1,list=FALSE)
forest.4.train1<-forest.4[intrain3,]
forest.4.test1<-forest.4[-intrain3,]

control <- trainControl(method="repeatedcv", number=5, repeats=3, search="grid")
set.seed(456)
tunegrid <- expand.grid(.mtry=c(1:15))
#rf_gridsearch <- train(CoverType ~ Elevation + Aspect + Slope + HDist.Hydrology +  
     #               + VDist.Hydrology + HDist.Roadway + Hillshade.9am + Hillshade.12pm  
      #              + Hillshade.3pm + HDist.FirePoint + Area1 + Area2 
     #               + Area3+ Area4 + trans.LDist.Hydrology + interElAr1 + interElAr2 
     #               + interElAr3 + interElAr4 + below.water + soil.pca.pc1 + soil.pca.pc2
     #               + soil.pca.pc3 + soil.pca.pc4 +  soil.pca.pc5 + soil.pca.pc6 + soil.pca.pc7
     #               + soil.pca.pc8 + soil.pca.pc9+  soil.pca.pc10+ soil.pca.pc11
     #               + soil.pca.pc12 + soil.pca.pc13+ soil.pca.pc14+ soil.pca.pc15
     #               + soil.pca.pc16, 
     #               data=forest.4.train1, method="rf", metric="Kappa", tuneGrid=tunegrid, trControl=control)
#saveRDS(rf_gridsearch, "./rf_gridsearch.rds")

rf_gridsearch <- readRDS("rf_gridsearch.rds")

plot(rf_gridsearch)


pred.rf2 = predict(rf_gridsearch$finalModel, newdata=forest.4.test1 )
cm.rf2 = confusionMatrix(data=pred.rf2, forest.4.test1$CoverType)
library(pander)
pander(cm.rf2$table)
#cm.rf2
f1 <- as.data.frame(cm.rf2$byClass[,7])
colnames(f1) <- "F1"
f1$Class <- row.names(f1)
ggplot(data=f1, aes(x=Class, y = F1)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("F1 Scores")

ba <- as.data.frame(cm.rf2$byClass[,11])
colnames(ba) <- "Balanced_Accuracy"
ba$Class <- row.names(ba)
ggplot(data=ba, aes(x=Class, y = Balanced_Accuracy)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("Balanced Accuracy")
## set.seed(444)
## forest.model.lda1 = forest.train4
## idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.lda1))
## forest.model.lda1[,idxs] = scale(forest.model.lda1[,idxs], center = TRUE, scale = TRUE)
## 
## 
## 
## #forest.lda1 <- train(CoverType ~ .,  data = forest.model.lda1,
##                              method = "lda",
##                              metric = "Kappa",
##                              preProc = c("center", "scale"))
## 
#saveRDS(forest.lda1, "./forest.lda1.rds")

forest.lda1 <- readRDS("./forest.lda1.rds")
forest.model.lda1.valid = forest.valid4
idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.lda1.valid))
forest.model.lda1.valid[,idxs] = scale(forest.model.lda1.valid[,idxs], center = TRUE, scale = TRUE)


pred.lda1 = predict(forest.lda1, newdata=forest.model.lda1.valid)
forest.lda1.cm = confusionMatrix(data=pred.lda1, forest.model.lda1.valid$CoverType)
forest.lda1.kappa = forest.lda1.cm$overall[2]
forest.lda1.accuracy = forest.lda1.cm$overall[1]

pander(forest.lda1.cm$table)
f1 <- as.data.frame(forest.lda1.cm$byClass[,7])
colnames(f1) <- "F1"
f1$Class <- row.names(f1)
ggplot(data=f1, aes(x=Class, y = F1)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("F1 Scores")

ba <- as.data.frame(forest.lda1.cm$byClass[,11])
colnames(ba) <- "Balanced_Accuracy"
ba$Class <- row.names(ba)
ggplot(data=ba, aes(x=Class, y = Balanced_Accuracy)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("Balanced Accuracy")
## library(class)
## library(caret)
## library(knncat)
## 
## 
## # temp make training set smaller.
## set.seed(444)
## forest.train.knn <- na.omit(forest.train4)
## forest.valid.knn <- na.omit(forest.valid4)
## 
## #prep data for centering and scaling by removing qualititative variables
## forest.train.x <- forest.train.knn[,-c(4,25,26)]
## forest.train.y <- forest.train.knn[,4]
## forest.valid.x <- forest.valid.knn[,-c(4,25,26)]
## forest.valid.y <- forest.valid.knn[,4]
## 
## 
## #normalize and standardize training data
## df.mean.train <- apply(forest.train.x, 2, mean)
## df.sd.train <- apply(forest.train.x, 2, sd)
## df.std.train <- t((t(forest.train.x)-df.mean.train)/df.sd.train) #standardize to have 0 mean and unit sd
## #apply(df.std.train, 2, mean) # check zero mean
## #apply(df.std.train, 2, sd) # check unit sd
## 
## forest.train.knn3 <-df.std.train
## forest.train.knn3 <- cbind(df.std.train,
##                        forest.train.knn$CoverType,
##                        forest.train.knn$forest.area,
##                        forest.train.knn$forest.zone)
## colnames(forest.train.knn3)[24] <- "CoverType"
## colnames(forest.train.knn3)[25] <- "forest.area"
## colnames(forest.train.knn3)[26] <- "forest.zone"
## forest.train.knn3 <- as.data.frame(forest.train.knn3)
## forest.train.knn3$CoverType <- as.factor(forest.train.knn3$CoverType)
## forest.train.knn3$forest.area <- as.factor(forest.train.knn3$forest.area)
## forest.train.knn3$forest.zone <- as.factor(forest.train.knn3$forest.zone)
## 
## #normalize and standardize validation data
## df.mean.valid <- apply(forest.valid.x, 2, mean)
## df.sd.valid <- apply(forest.valid.x, 2, sd)
## df.std.valid <- t((t(forest.valid.x)-df.mean.valid)/df.sd.valid) #standardize to have 0 mean and unit sd
## #apply(df.std.valid, 2, mean) # check zero mean
## #apply(df.std.valid, 2, sd) # check unit sd
## 
## forest.valid.knn3 <- df.std.valid
## forest.valid.knn3 <- cbind(df.std.valid,
##                            forest.valid.knn$CoverType,
##                            forest.valid.knn$forest.area,
##                            forest.valid.knn$forest.zone)
## colnames(forest.valid.knn3)[24] <- "CoverType"
## colnames(forest.valid.knn3)[25] <- "forest.area"
## colnames(forest.valid.knn3)[26] <- "forest.zone"
## forest.valid.knn3 <- as.data.frame(forest.valid.knn3)
## forest.valid.knn3$CoverType <- as.factor(forest.valid.knn3$CoverType)
## forest.valid.knn3$forest.area <- as.factor(forest.valid.knn3$forest.area)
## forest.valid.knn3$forest.zone <- as.factor(forest.valid.knn3$forest.zone)
## 
## forest.train.knn3.x <- forest.train.knn3[,-c(24)]
## forest.train.knn3.y <- forest.train.knn3[24]
## forest.valid.knn3.x <- forest.valid.knn3[,-c(24)]
## forest.valid.knn3.y <- forest.valid.knn3[24]
## 
## 
## #K-Fold Cross Validation
## set.seed(1)
## idx <- createFolds(forest.train.y , k=10)
## #sapply(idx, length)
## 
## #head(forest.train.knn[,4])
## #head(forest.train.knn3[24])
## 
## 
## #Optimize K
## ks <- 1:12
## #res <- sapply(ks, function(k) {
## #  ##try out each version of k from 1 to 12
## #  res.k <- sapply(seq_along(idx), function(i) {
## #    ##loop over each of the 10 cross-validation folds
## #    ##predict the held-out samples using k nearest neighbors
## #    knn.pred <- knn(forest.train.knn3.x[-idx[[1]] , ],
## #                    forest.valid.knn3.x[ idx[[1]], ],
## #                    forest.train.y[-idx[[1]] ],
## #                    k = k)
## #    ##the ratio of misclassified samples
## #    mean(forest.valid.y[ idx[[i]] ] != knn.pred)
## #  })
##   ##average over the 10 folds
## #  mean(res.k)
## #})
## #res
## 
## #df.knn <- knn(forest.train.knn3.x,forest.valid.knn3.x,forest.train.y,k=9)
## #saveRDS(df.knn, "df.knn1.rds")
## df.knn1 <- readRDS("df.knn1.rds")
## 
## #knn.confusion <- table(df.knn1,forest.valid.y[ idx[[1]] ] )
## knn.confusion <- table(df.knn1,forest.valid.y)
## 
## #plot(ks, res, type="o",ylab="misclassification error") #K=9
## forest.knn1.cm<-confusionMatrix(knn.confusion)
## 
## forest.knn1.kappa <- forest.knn1.cm[2]
## forest.knn1.accuracy <-forest.knn1.cm[1]
## #lr.mod1 <- train(CoverType ~ ., data=forest.4.train, method="multinom")
## 
#saveRDS(lr.mod1, "lr.mod1.rds")
lr.mod1 <- readRDS("lr.mod1.rds")
## #Predict on Validation Set
## pred.lr.mod1 <- predict.train(lr.mod1, newdata = forest.valid4)
## xtab <- table(pred.lr.mod1, forest.valid4$CoverType )
#Predict on Validation Set
pred.lr.mod1 <- predict.train(lr.mod1, newdata = forest.4.test)
xtab <- table(pred.lr.mod1, forest.4.test$CoverType ) 
lr.mod1.cm <- confusionMatrix(xtab)
#lr.mod1.cm
library(e1071)
set.seed(444)
forest.model.svm1 = sample_n(forest.train4, 40000)
idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.svm1))
forest.model.svm1[,idxs] = scale(forest.model.svm1[,idxs], center = TRUE, scale = TRUE)

#forest.svm.1 <- svm(CoverType ~ ., method = "class", data = forest.model.svm1)
#forest.svm.1
#saveRDS(forest.svm.1, "./forest.svm1.rds")
forest.svm1 <- readRDS("./forest.svm1.rds")
forest.model.svm1.valid = forest.valid4
idxs = grep("Elevation|HDist|trans|shade",colnames(forest.model.svm1.valid))
forest.model.svm1.valid[,idxs] = scale(forest.model.svm1.valid[,idxs], center = TRUE, scale = TRUE)


#pred.svm1 = predict(forest.svm.1, newdata=forest.model.svm1.valid)

#saveRDS(pred.svm1, "./pred.svm1.rds")
pred.svm1 <- readRDS("./pred.svm1.rds")

tab.svm <- table("Predicted Class" = pred.svm1, "Actual" = forest.model.svm1.valid$CoverType)
#tab.svm
forest.svm1.cm <-confusionMatrix(data=pred.svm1, forest.model.svm1.valid$CoverType)
forest.svm1.kappa = forest.svm1.cm$overall[2]
forest.svm1.accuracy = forest.svm1.cm$overall[1]
pander(forest.svm1.cm$table)
f1 <- as.data.frame(forest.svm1.cm$byClass[,7])
colnames(f1) <- "F1"
f1$Class <- row.names(f1)
ggplot(data=f1, aes(x=Class, y = F1)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("F1 Scores")

ba <- as.data.frame(forest.svm1.cm$byClass[,11])
colnames(ba) <- "Balanced_Accuracy"
ba$Class <- row.names(ba)
ggplot(data=ba, aes(x=Class, y = Balanced_Accuracy)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("Balanced Accuracy")
#add K Nearest Neighbor and Logistic Regression

models = c("Neural Network- 15 Nodes","Random Forest","Random Forest w PCA", "Linear Discriminant Analysis","K Nearest Neighbors","Support Vector Machines")

stats = c("Model", "Accuracy", "Kappa")

accuracy = round(c(forest.nn5.accuracy,cm.rf1$overall[1],cm.rf2$overall[1],forest.lda1.accuracy,forest.knn1.cm$overall[1],forest.svm1.accuracy),
digits = 3)

kappa = round(c(forest.nn5.kappa,cm.rf1$overall[2],cm.rf2$overall[2],forest.lda1.kappa,forest.knn1.cm$overall[2],forest.svm1.kappa),digits = 3)

summary_table = cbind(models,accuracy,kappa)
colnames(summary_table) = stats
rownames(summary_table) = NULL
pander(summary_table, justify = "left")
ba <- as.data.frame(cm.rf2$byClass[,11])
colnames(ba) <- "Balanced_Accuracy"
ba$Class <- row.names(ba)
ggplot(data=ba, aes(x=Class, y = Balanced_Accuracy)) +
  geom_bar(stat="identity", width=0.5) + ggtitle("Balanced Accuracy")
## library(randomForest)
## library(caret)
## 
## library(doMC)
## registerDoMC(cores = 5)
## 
## # temp make training set smaller.
## set.seed(444)
## forest.train = forest.train[sample(nrow(forest.train), 20000, replace = FALSE),]
## 
## # setup
## preprocess = c("zv")
## control = trainControl(method="cv", number=10)
## metric = "Kappa"
## set.seed(444)
## mtry = floor(sqrt(ncol(forest.train) - 1))
## tunegrid = expand.grid(mtry=mtry, ntree=c(500,1000,1500))
## 
## idxs = grep("^orig.Elevation|^orig.Aspect|^orig.Slope|^orig|^VDist|^Hillshade|^Area|^SoilType",colnames(forest.train))
## form = sapply(colnames(forest.train[,idxs]),function(x) {
##   return (paste(x,' + '))
## })
## form = trimws(paste(unlist(form), collapse=''))
## form = trimws(gsub('\\+$','',form))
## form = paste('CoverType ~ ',form)
## form = as.formula(form)
## 
## # model.rf <- train(form,
## #                   data      = forest.train,
## #                   method    = "rf",
## #                   metric    = metric,
## #                   tuneGrid  = tunegrid,
## #                   trControl = control,
## #                   preProc   = preprocess)
## 
## df.rf = randomForest(CoverType~.,data=forest.train,mtry=9,importance=TRUE)
## varImpPlot(df.rf, main='Variable Importance from Random Forest', cex=0.8)
## 
